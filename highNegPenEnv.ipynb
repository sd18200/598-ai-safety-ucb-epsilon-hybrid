{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gym numpy"
      ],
      "metadata": {
        "id": "sCefv96KkcGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXioksNJkLgV"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class MazeEnvironment(gym.Env):\n",
        "    def __init__(self, exploration_strategy='epsilon_greedy'):\n",
        "        super(MazeEnvironment, self).__init__()\n",
        "\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        self.observation_space = spaces.Tuple((spaces.Discrete(5), spaces.Discrete(5)))\n",
        "\n",
        "        self.initialize_maze()\n",
        "\n",
        "        self.q_table = np.zeros((25, 4))  # 25 states and 4 actions\n",
        "        self.epsilon = 1.0  # Initial exploration rate for epsilon-greedy\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.95\n",
        "        self.alpha = 0.1  # Initial learning rate\n",
        "        self.gamma = 0.99  # Discount factor\n",
        "        self.last_episode_steps = np.inf  # Track steps of the last episode\n",
        "        self.threshold_nse = 50  # Threshold for NSE mitigation\n",
        "        self.nse_encounter_count = 0  # Counter for NSE encounters\n",
        "        self.nse_encounters = []  # Track NSE encounters per episode\n",
        "        self.exploration_strategy = exploration_strategy  # Exploration strategy: 'epsilon_greedy' or 'ucb'\n",
        "        self.ucb_c = 2\n",
        "\n",
        "    def initialize_maze(self):\n",
        "        self.goal_state = (4, 4)\n",
        "        self.start_state = (0, 0)\n",
        "        self.agent_position = self.start_state\n",
        "        self.forbidden_zones = {(1, 1), (1, 2)}\n",
        "        self.blocked_paths = {(2, 2), (3, 2)}\n",
        "        self.step_penalty_threshold = 10\n",
        "        self.steps_taken = 0\n",
        "        self.total_reward = 0\n",
        "\n",
        "        self.maze = np.zeros((5, 5), dtype=str)\n",
        "        for r in range(5):\n",
        "            for c in range(5):\n",
        "                self.maze[r, c] = '.'\n",
        "        self.maze[self.goal_state] = 'G'\n",
        "        for fz in self.forbidden_zones:\n",
        "            self.maze[fz] = 'F'\n",
        "        for bp in self.blocked_paths:\n",
        "            self.maze[bp] = 'B'\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_position = self.start_state\n",
        "        self.steps_taken = 0\n",
        "        self.total_reward = 0\n",
        "        self.update_maze_position(self.start_state, self.agent_position)\n",
        "        return self.state_to_index(self.agent_position)\n",
        "\n",
        "    def state_to_index(self, state):\n",
        "        return state[0] * 5 + state[1]\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.exploration_strategy == 'epsilon_greedy' and np.random.random() < self.epsilon:\n",
        "            action = self.action_space.sample()\n",
        "\n",
        "        next_state = list(self.agent_position)\n",
        "        if action == 0 and next_state[0] > 0: next_state[0] -= 1\n",
        "        elif action == 1 and next_state[0] < 4: next_state[0] += 1\n",
        "        elif action == 2 and next_state[1] > 0: next_state[1] -= 1\n",
        "        elif action == 3 and next_state[1] < 4: next_state[1] += 1\n",
        "\n",
        "        next_state = tuple(next_state)\n",
        "        reward, done = self.calculate_reward(next_state)\n",
        "        self.total_reward += reward\n",
        "        self.steps_taken += 1\n",
        "\n",
        "        current_index = self.state_to_index(self.agent_position)\n",
        "        next_index = self.state_to_index(next_state)\n",
        "        best_future_reward = np.max(self.q_table[next_index])\n",
        "\n",
        "        # Implement UCB exploration strategy\n",
        "        if self.exploration_strategy == 'ucb':\n",
        "            if np.sum(self.q_table[current_index]) > 0:\n",
        "              exploration_bonus = self.ucb_c * np.sqrt(np.log(self.steps_taken + 1) / np.sum(self.q_table[current_index]))\n",
        "            else:\n",
        "              exploration_bonus = self.ucb_c * np.sqrt(np.log(self.steps_taken + 1) / 1e-6)\n",
        "\n",
        "            ucb_q_values = self.q_table[current_index] + exploration_bonus\n",
        "            action_ucb = np.argmax(ucb_q_values)\n",
        "\n",
        "            if np.random.random() < self.epsilon:\n",
        "                action = action_ucb\n",
        "\n",
        "        self.q_table[current_index, action] += self.alpha * (reward + self.gamma * best_future_reward - self.q_table[current_index, action])\n",
        "\n",
        "        self.update_maze_position(self.agent_position, next_state)\n",
        "        self.agent_position = next_state\n",
        "\n",
        "        if done:\n",
        "            if self.exploration_strategy == 'epsilon_greedy':\n",
        "                self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
        "            elif self.exploration_strategy == 'ucb':\n",
        "                # No need to update epsilon for UCB strategy\n",
        "                pass\n",
        "\n",
        "            if self.steps_taken > self.last_episode_steps:\n",
        "                self.total_reward -= 10  # Penalty for more steps\n",
        "            elif self.steps_taken < self.last_episode_steps:\n",
        "                self.total_reward += 10  # Reward for fewer steps\n",
        "            self.last_episode_steps = self.steps_taken  # Update last episode steps\n",
        "\n",
        "            if self.total_reward < -self.threshold_nse:\n",
        "                self.total_reward -= 50  # Additional penalty for exceeding NSE threshold\n",
        "\n",
        "            self.nse_encounters.append(self.nse_encounter_count)\n",
        "            self.nse_encounter_count = 0  # Reset NSE encounter count for next episode\n",
        "\n",
        "        return self.state_to_index(next_state), reward, done, {}\n",
        "\n",
        "    def calculate_reward(self, next_state):\n",
        "        if next_state in self.forbidden_zones:\n",
        "            self.nse_encounter_count += 1\n",
        "            return -50, True\n",
        "        elif next_state in self.blocked_paths:\n",
        "            self.nse_encounter_count += 1\n",
        "            return -20, False\n",
        "        elif next_state == self.goal_state:\n",
        "            return 100, True\n",
        "        elif self.steps_taken % self.step_penalty_threshold == 0:\n",
        "            self.nse_encounter_count += 0\n",
        "            return -10, False\n",
        "        return -5, False\n",
        "\n",
        "    def update_maze_position(self, old_pos, new_pos):\n",
        "        self.maze[old_pos] = '.'\n",
        "        self.maze[new_pos] = 'A'\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(\"\\n\".join([\"\".join(row) for row in self.maze]))\n",
        "\n",
        "class HybridMazeEnvironment(MazeEnvironment):\n",
        "    def __init__(self):\n",
        "        super(HybridMazeEnvironment, self).__init__()\n",
        "        self.ucb_c = 2  # UCB exploration parameter\n",
        "        # Initialize separate Q-tables for Epsilon-Greedy and UCB strategies\n",
        "        self.q_table_greedy = np.zeros((25, 4))  # Q-table for epsilon-greedy\n",
        "        self.q_table_ucb = np.zeros((25, 4))  # Q-table for UCB\n",
        "        self.epsilon_greedy_epsilon = 1.0\n",
        "        self.epsilon_greedy_decay = 0.95\n",
        "\n",
        "\n",
        "def step(self, action):\n",
        "    current_index = self.state_to_index(self.agent_position)\n",
        "\n",
        "    # Compute the next state based on the action taken\n",
        "    next_state = list(self.agent_position)\n",
        "    if action == 0 and next_state[0] > 0:\n",
        "        next_state[0] -= 1\n",
        "    elif action == 1 and next_state[0] < 4:\n",
        "        next_state[0] += 1\n",
        "    elif action == 2 and next_state[1] > 0:\n",
        "        next_state[1] -= 1\n",
        "    elif action == 3 and next_state[1] < 4:\n",
        "        next_state[1] += 1\n",
        "\n",
        "    next_state_tuple = tuple(next_state)\n",
        "\n",
        "    # Perform action based on selected strategy\n",
        "    if np.random.random() < self.epsilon_greedy_epsilon:\n",
        "        strategy = 'epsilon_greedy'\n",
        "    else:\n",
        "        strategy = 'ucb'\n",
        "\n",
        "    if strategy == 'epsilon_greedy':\n",
        "        epsilon_action = np.argmax(self.q_table_greedy[current_index])\n",
        "        next_state, reward, done, info = super().step(epsilon_action)\n",
        "        self.update_q_table('greedy', current_index, epsilon_action, next_state_tuple, reward, done)\n",
        "        self.epsilon_greedy_epsilon = max(self.epsilon_greedy_epsilon * self.epsilon_decay, self.epsilon_min)\n",
        "    else:\n",
        "        if np.sum(self.q_table[current_index]) > 0:\n",
        "            exploration_bonus = self.ucb_c * np.sqrt(np.log(self.steps_taken + 1) / np.sum(self.q_table[current_index]))\n",
        "        else:\n",
        "            exploration_bonus = self.ucb_c * np.sqrt(np.log(self.steps_taken + 1) / 1e-6)\n",
        "\n",
        "        ucb_q_values = self.q_table[current_index] + exploration_bonus\n",
        "        ucb_action = np.argmax(ucb_q_values)\n",
        "        next_state, reward, done, info = super().step(ucb_action)\n",
        "        self.update_q_table('ucb', current_index, ucb_action, next_state_tuple, reward, done)\n",
        "\n",
        "    return next_state_tuple, reward, done, info\n",
        "\n",
        "def update_q_table(self, strategy, state_index, action, next_state, reward, done):\n",
        "    # Update Q-table based on the strategy and other parameters\n",
        "    next_index = self.state_to_index(next_state)\n",
        "    best_future_reward = np.max(self.q_table[next_index])\n",
        "\n",
        "    if strategy == 'greedy':\n",
        "        self.q_table_greedy[state_index, action] += self.alpha * (\n",
        "            reward + self.gamma * best_future_reward - self.q_table_greedy[state_index, action])\n",
        "    elif strategy == 'ucb':\n",
        "        self.q_table_ucb[state_index, action] += self.alpha * (\n",
        "            reward + self.gamma * best_future_reward - self.q_table_ucb[state_index, action])\n",
        "\n",
        "    self.update_maze_position(self.agent_position, next_state)\n",
        "    self.agent_position = next_state\n",
        "\n",
        "    if done:\n",
        "        self.epsilon_greedy_epsilon = max(self.epsilon_greedy_epsilon * self.epsilon_decay, self.epsilon_min)\n",
        "        if self.steps_taken > self.last_episode_steps:\n",
        "            self.total_reward -= 10  # Penalty for more steps\n",
        "        elif self.steps_taken < self.last_episode_steps:\n",
        "            self.total_reward += 10  # Reward for fewer steps\n",
        "        self.last_episode_steps = self.steps_taken  # Update last episode steps\n",
        "\n",
        "        if self.total_reward < -self.threshold_nse:\n",
        "            self.total_reward -= 50  # Additional penalty for exceeding NSE threshold\n",
        "\n",
        "        self.nse_encounters.append(self.nse_encounter_count)\n",
        "        self.nse_encounter_count = 0  # Reset NSE encounter count for next episode\n",
        "\n",
        "    return self.state_to_index(next_state), reward, done, {}\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    epsilon_greedy_env = MazeEnvironment(exploration_strategy='epsilon_greedy')\n",
        "    ucb_env = MazeEnvironment(exploration_strategy='ucb')\n",
        "    hybrid_env = HybridMazeEnvironment()\n",
        "    episodes = 100\n",
        "    rewards_epsilon_greedy = []\n",
        "    steps_epsilon_greedy = []\n",
        "    nse_encounters_epsilon_greedy = []\n",
        "    rewards_ucb = []\n",
        "    steps_ucb = []\n",
        "    nse_encounters_ucb = []\n",
        "    rewards_hybrid = []\n",
        "    steps_hybrid = []\n",
        "    nse_encounters_hybrid = []\n",
        "\n",
        "    for e in range(episodes):\n",
        "        # Run epsilon-greedy simulation\n",
        "        state_epsilon_greedy = epsilon_greedy_env.reset()\n",
        "        done_epsilon_greedy = False\n",
        "        while not done_epsilon_greedy:\n",
        "            action_epsilon_greedy = np.argmax(epsilon_greedy_env.q_table[epsilon_greedy_env.state_to_index(epsilon_greedy_env.agent_position)])\n",
        "            state_epsilon_greedy, reward_epsilon_greedy, done_epsilon_greedy, info_epsilon_greedy = epsilon_greedy_env.step(action_epsilon_greedy)\n",
        "        rewards_epsilon_greedy.append(epsilon_greedy_env.total_reward)\n",
        "        steps_epsilon_greedy.append(epsilon_greedy_env.steps_taken)\n",
        "        nse_encounters_epsilon_greedy.append(epsilon_greedy_env.nse_encounters[-1])\n",
        "\n",
        "        # Run UCB simulation\n",
        "        state_ucb = ucb_env.reset()\n",
        "        done_ucb = False\n",
        "        while not done_ucb:\n",
        "            action_ucb = np.argmax(ucb_env.q_table[ucb_env.state_to_index(ucb_env.agent_position)])\n",
        "            state_ucb, reward_ucb, done_ucb, info_ucb = ucb_env.step(action_ucb)\n",
        "        rewards_ucb.append(ucb_env.total_reward)\n",
        "        steps_ucb.append(ucb_env.steps_taken)\n",
        "        nse_encounters_ucb.append(ucb_env.nse_encounters[-1])\n",
        "\n",
        "        # Run Hybrid simulation\n",
        "        state_hybrid = hybrid_env.reset()\n",
        "        done_hybrid = False\n",
        "        while not done_hybrid:\n",
        "            action_hybrid = np.argmax(hybrid_env.q_table[hybrid_env.state_to_index(hybrid_env.agent_position)])\n",
        "            state_hybrid, reward_hybrid, done_hybrid, info_hybrid = hybrid_env.step(action_hybrid)\n",
        "        rewards_hybrid.append(hybrid_env.total_reward)\n",
        "        steps_hybrid.append(hybrid_env.steps_taken)\n",
        "        nse_encounters_hybrid.append(hybrid_env.nse_encounters[-1])\n",
        "\n",
        "        print(f\"Episode {e+1} (Epsilon-Greedy): Total Reward = {epsilon_greedy_env.total_reward}, Steps = {epsilon_greedy_env.steps_taken}\")\n",
        "        print(f\"Episode {e+1} (UCB): Total Reward = {ucb_env.total_reward}, Steps = {ucb_env.steps_taken}\")\n",
        "        print(f\"Episode {e+1} (Hybrid): Total Reward = {hybrid_env.total_reward}, Steps = {hybrid_env.steps_taken}\")\n",
        "\n",
        "    # Plotting the results\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(rewards_epsilon_greedy, label='Epsilon-Greedy')\n",
        "    plt.plot(rewards_ucb, label='UCB')\n",
        "    plt.plot(rewards_hybrid, label='Hybrid')\n",
        "    plt.title('Total Rewards per Episode')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(steps_epsilon_greedy, label='Epsilon-Greedy')\n",
        "    plt.plot(steps_ucb, label='UCB')\n",
        "    plt.plot(steps_hybrid, label='Hybrid')\n",
        "    plt.title('Steps per Episode')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Steps Taken')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot NSE encounters vs. episodes\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, episodes + 1), nse_encounters_epsilon_greedy, label='Epsilon-Greedy')\n",
        "    plt.plot(range(1, episodes + 1), nse_encounters_ucb, label='UCB')\n",
        "    plt.plot(range(1, episodes + 1), nse_encounters_hybrid, label='Hybrid')\n",
        "    plt.title('Negative Penalties Encounters per Episode')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Negative Penalties Encounters')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    }
  ]
}